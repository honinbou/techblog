---
layout: post
title: amazon 2012-10-22 事故分析总结
category: knowledge
tags: [pyinotify]
---

amazon在2012年10月22在美国东海岸发生了服务故障，他们的团队事后进行了分析和总结，本文是其英文版的翻译，[原文点击这里][english-link]

# 事故描述
 
1. 星期一10:00AM PDT, 东海岸5个区域中的一个Zone发生少量的Elastic Block Store (EBS)服务拒绝，具体表现为无法进行IO请求,原因为agent在某个异常状态下会发生内存泄漏，而这个内存泄漏是和请求相关的；
2. 服务拒绝的EBS机器到达了某个临界值，开始发生雪崩，很快更多的机器无法处理用户请求，导致更多的机器无法进行IO请求；
3. 11:00AM左右，这个Zone中的大部分EBS volume无法使用；
4. 11:15AM左右，为了恢复服务，团队决定减少故障转移率，采取限制流量的措施，防止更多机器雪崩；
5. 11:35AM，系统开始自动恢复部分EBS 存储volume；
6. 1:40PM，60%受影响的volume被恢复，团队继续查找原因，并继续恢复余下有问题的volume。由于故障转移和恢复的量太大，导致团队很难真正定位问题；
7. 3:10PM，团队定位出真正的问题，并通过释放agent多占的内存来恢复剩下大部分未被恢复的volume；
8。4:15PM，所有受影响的存储volume均被恢复至正常状态

# 原因

1. EBS 服务拒绝原因：
* EBS 存储服务器上收集数据的agent有一个潜在的bug, 个人理解agent的作用类似于Mola的agent，个人猜测该agent既可以读数据，也可以写数据。agent设计为对时间不敏感和对数据的延时和缺失有容错性。
* 上周，其中一台存储机器(chunk)硬件故障并被替换，同时内部DNS(可能用做寻地址，这里估计agent查找后端的server是通过域名，而不是ip来查找)进行更新。
* 因为没有注意到，内部DNS并没有完成更新记录成功，导致少部分机器继续连接已经下架的机器，而不是更新后的好机器，因为架构设计允许数据缺失，这个问题并没有立即触发报警.
* 但这种重试，触发了reporting agent(和刚刚的agent是一个？)上一个隐藏的内存泄漏的bug。而且这个泄漏是和重试次数相关的，这样就导致agent内存持续的被消耗。
* amazon有监控每个EBS server的总的内存消耗，但是这种消耗，监控系统并没有报警，EBS 由于需要处理用户的数据，其使用的内存差异很大，导致无法设置关于内存使用量的报警策略。
* 周一早上，由于内存泄漏已经非常高了，导致无法正常的处理用户的正常请求。

2. 雪崩的原因
* 所有云服务的冗余设计，将请求从故障机器转移到服务OK的机器上。由于同时内存泄漏的机器太多，导致没有足够多服务状态OK的机器能够处理转移请求，同时由于内存泄漏的机器更多，导致更多的agent内存泄漏，就导致更多的机器无法服务。

# 后期的防备措施

1. 针对这种内存泄漏，在所有EBS服务器上进行监控；
2. 下周开始，对所有的EBS服务器(包括不同地区)部署新的fix版本；
3. 修改系统监控，针对EBS服务器上**每个进程**进行内存消耗的监控；
4. 后续部署资源限制，阻止低优先级别的进程过多的使用机器上的资源；
5. 更新内部DNS配置，确保每次DNS的更新都是可靠的到达每台机器；更重要的是，针对这种未更新成功的情况，监控系统能够做出及时的报警；

上面的这些措施，均能及早的定位引发这次事故的原因。另外，还需要评估如何修改EBS故障转移的逻辑，以避免类似这次快速的大范围的雪崩情况

# 造成的影响

## 对EC2和EBS APIs 的影响

* 描述: 这个事故只影响了一个Zone的EBS volume服务，如果用户在其它的Zone上有足够的空间，理论上其所受的影响不大。amazon投入了大量的精力来避免service APIs受到单个Zone出事故而造成的影响。但实际上仍然有用户抱怨在这次事故中，无法进行IO操作，而此时amazon的监控系统显示EBS APIs服务正常。原因为Api的限流，并没有均匀的分布到所有的用户，造成部分用户被Api限流过于限制导致无法操作资源
* amazon使用限流来保证内部或者外部，恶意或者非恶意的大量访问，参见[SteveY的吐槽][steveY对amazon和google的吐槽],[英文版][SteveY rant]，amazon的平台在设计之初，就非常关注配额和限流。对于单个应用，在获取某个正确结果前的多次重试，平台是可以处理的。但是在这次事故中，所有的application一起发生故障，导致对平台产生了大量的负载，amazon有一个基础的限制浏览的方案。为了保证恢复过程中其他正常机器服务的稳定和不正常机器的恢复，团队制定了非常激进的限流措施，后面证实这个措施过于激进了。
* 原因: 12:06PM PDT 团队执行了这个激进的限流措施，同时在监控着整体的流量和其他活动，由于监控显示正常，并没有意识到用户已经受到了显著的影响。后来意识到，少部分用户在这个限流策略下，被极高比例的限制Api调用，这些Api调用维持了大量的用户在调用他们的EC2实例和EBS资源，由于这些用户被限制的非常厉害，导致他们在事故中调用APIS服务很难成功，同时影响到他们通过AWS的管理平台来管EC2服务和EBS资源，直到 2:33PM PDT，团队才显著的减少了限流的程度。
* 后续的改进: 1. 故障处理中减少使用这种激进的处理策略，使用其它即能够恢复服务，又减少用户感知的限流策略；2. 操作dashbord上增加精细到用户级别的限流监控，而不仅仅是整体的限流监控，以减少这种情况再次发生，同时做出最准确的判断。
* 限流是维持服务健康非常重要的工具，同时由于用户有经常更改限流强度的需求（amazon提供按需服务），amazon在不影响用户使用服务的前提下部署使用。从这次事故中，限流措施对许多用户造成的影响远超过团队的最初的意识。这次限流中，没有明显影响到跨多个Zone的程序，但影响到了许多单Zone的用户，导致他们在几个无法恢复到健康状态，甚至是正常工作的down。后面就是赔偿balala的。。。

## 对RDS(Amazon Relational Database Service)的影响

* 描述：因为RDS使用EBS来存储database和log，所以部分在受影响区域的RDS database在这次事故中也悲剧了。当然，使用其它Zone的RDS instance是不受影响的。
* amazon的RDS有两种使用模式：Single Availability Zone (Single-AZ)单区域(唯一的一个single DataBase instance in one Zone)和Multi Availability Zone (Multi-AZ)多区域(两个database instance in two different Zone)，类似于mysql的主从模式，其中一个为主一个为从，正常情况下，主database hold住所有的请求，并复制到从上，在主挂掉时，从完成完整性check后，迅速的变为主。
* Single-AZ的database实例在受影响的Zone中被直接影响了。在事故中，如果database实例使用的一个EBS volume有问题，则会被影响到，具体表现为IO请求异常(如上所描述)，Single-AZ的恢复，依赖于其底层使用的EBS volume的恢复。1:30PM PDT, 很多受影响的Single-AZ RDS instance被恢复，3:30PM PDT, 主要的受影响的database instance 被恢复，6:35PM PDT, 几乎所有的Single-AZ RDS instances 恢复。
* 大部分的Multi-AZ在完成一致性check后，能够正常的切换到健康的Zone的从上，但是由于存在两个完全不同的bug（吐槽一下,bug真心多啊，哈哈），低于10%的Mulit-AZ database instance没有自动的完成故障转移。
	1. 部分instance没有正确转移是因为碰到了一个不寻常的IO操作，导致转移逻辑没有正常处理，这些instance需要操作动作，并且存储的时间是11:30AM（个人估计是故障转移时需要写IO操作，而IO操作刚好是在11:30AM这个有问题的时候，导致异常，没太看明白这个bug）
	2. 第二个bug的触发条件:a.master database instance 和从的连接断了一个很短的时间；b. 此时master的database instance写volume失败，两者几乎同时进行；c. 从master和slave连接丢失到master database写volume失败这段时间内，master还在处理业务，而没有将其转移到其从上；在master的IO请求无法处理时，此时系统因为slave的数据是过时的数据（integrity check 失败）而被block住了。后续的修改：在master的Zone出现问题开始时，立马进行故障转移；由于问题涉及比较广,估计整个要到12月才能修复并完全上线。database instance在EBS volume恢复后就恢复了。在这两个bug修复后，预计Multi-AZ的问题就会被修复，这两个问题是团队后面最高的优先级,balala...
* 如果一个application是Multi-AZ，同时在一个Zone不可用后，仍然有足够的资源来进行后续操作的话，那么这个程序应该是可用的。后面又是赔偿等一些balala的...	


## 对ELB(Amazon Elastic Load Balancing)的影响

* 每个ELB load balancer 使用一个或者多个load balancer 实例做用户EC2实例的路由。ELB使用EBS来存储配置和监控信息。当ELB实例使用的EBSvolume被hung住的时候，部分ELB服务就变得不可用，此时ELB服务开始启动恢复流程来恢复或者替代有问题的load balancer实例。
用户的application中使用的ELB服务可以是位于1个或者多个Zone的ELB服务
* 如果用户的ELB绑定的application运行在一个Zone中，ELB的load balancer实例和application被设定运行在同一个区域。在这次事故中，部分Single-AZ load balancer的服务由于部分的load balancer实例无法从EBS中进行IO操作而发生服务故障。这些影响在ELB system能够从受影响的Zone中获取到可用的EBS volume后迅速恢复了。1:10PM PDT, 主要受影响的单Zone的load balancer都被恢复；3:30PM PDT，大部分的的load balancers被恢复；最后一批load balancer被恢复很慢，是由于ELB的恢复策略导致的问题。ELB是用弹性IP池(Elastic IP addresses (EIPs))来可靠的将流量路由到EC2的实例上。EIPs被新的load balancer实例和恢复的load balancer消耗，同时原有的ELB的EIPs未被释放掉，导致所有的EIPs被全部消耗完毕。团队后来手动修复了最后一批失败的load balancer的实例，修复了EIPs的短缺，9:50PM PDT 完成全部修复
* 后续改进：
	1. 减少ELB的恢复时间；
	2. 保证每个Zone在恢复的时候有足够的EIPs；
	3. 较少ELB和EBS间的耦合，允许在一个Zone内即使EBS有问题，ELB仍然可以恢复；
	4. 后续添加若干ELB的恢复流程的改进来改善类似事故中ELB的恢复时间;

* 如果用户的ELB绑定的application运行在多个Zone中，ELB会为application运行的每个区域设置一个load balancer实例，ELB会自动将服务质量降低区域的流量切走，以保证服务的快速恢复。在这次事故中，多Zone的ELB服务，会明显感受到访问错误率的升高。11:49AM PDT, 大部分的ELB service 从受影响的区域切走到那些未受影响的区域。不幸的是，流量切换函数中的一个bug，导致少部分受影响的load balancer没有被正确的map，从而导致流量没有被正常的切走。这些load balancer持续的发送用户的请求到受影响的Zone。直到12:45PM这个问题被正确的定位。
* 改进:
	1. 修复了这个逻辑bug，保证以后不会再出现；
	2. 采取若干措施提高流量切换过程的灵敏度，使得将来流量能更快的从受影响的区域切走;
	3. 将流量切换函数保留给用户，使得他们可以自己控制将流量切换到可用的区域;
	4. 协助用户使用我们的流量迁移策略，是的他们可以自己处理这种一个Zone失败的情况(按照平台的设计思想，这个应该是对用户透明的，个人觉得是在问题发生的时候，amazon没有足够的人手来处理这个问题，而这个对用户的影响很大，所以很多用户要求有自己处理流量转移的权限)；

# AWS目前能够完成的功能
1. 优点：
	* 配额和节流，amazon的配额和节流是事故从雪崩的状态慢慢恢复的最重要手段；
	* 无处不在的监控，从上面的描述来看，他们的监控已经做到整体上的各个角度监控，后续监控可能会细粒度到用户这一层；
	* 自动化，自动化故障迁移，自动化的流量切换，听说amazon可能是IT公司中自动化工具最多的公司。。。

2. 缺点：
	* bug有点多啊，这个事故中共有四个bug，当然是不同的产品线，也是非常难以触发的bug。可能云的产品测试，需要将异常情况作为常态来测试，同时要真正的模拟异常的场景，如果是手动测试，成本很难接受，尤其是不停的迭代；如果是自动化测试，对技术要求非常高。成本也不低；
	* 产品线的耦合比较高。。。这个不知道算不算缺点，按道理来说应该是尽可能的复用已有的产品，但是复用别人的产品，应该做好依赖的产品不可靠的最坏准备。

# 之前BCS已经完成的功能
1. 优点:
	* BCS对内和对外做了两套系统，对内的是不限制额度和流量的。对外的是精确限制额度和流量的，既可以整体限制，也可以精确到bucket级别，还有针对特殊bucket的特殊策略(内部版)，这个地方其实可以从一个更高更统一的角度进一步改进。
	* 相对来说BCS的监控可能做的可能要简单些。BCS的监控有：基础项的监控(不知道有没有精确到进程级别的监控，如内存等等)，错误日志的监控，状态服务码的监控，bucket基本的监控(但只有top*** bucket的监控)，上层的语义监控不知道现在做了没。其实如果再次设计一个BCS，监控的优先级可以提高，投入多一些精力，界面做的更加人性化。
	* BCS的内部模块通信都有重试机制，但是这个离真正的自动化还是有较远的距离。自动化运维，故障转移自动化，貌似最近在上多Zone，多Zone间的通讯，同步，流量自动迁移。。。技术挑战还是非常高的。

2. 缺点：
	* bug也很多，惭愧一下，基本上面发生的bug，差不多都会经历的吧，比如：BCS中一个模块在异常连接失败后的内存泄漏，我自己漏测过。。。 
	* BCS也和其他产品存在耦合，比如：BAE，但是如果BCS全部down掉，BAE只有无法发布新应用的影响，而不会导致BAE服务停止。当然，如果应用有写BCS的请求，仍然会有影响。BCS也有部分模块运行在BAE上，但不是核心功能。其他新的业务，不清楚他们的耦合怎么样。

# 感想
* 从之前做BCS到后面的PCS，到学习这个事故，发现云计算机器数目的增加，不仅仅只是原来技术方案的扩倍，它会带来很多新问题需要解决，会带来更多新的技术挑战。在少量机器下不明显的问题，在大量机器下，可能要花很多经历去解决。一个简单的类比例子是：一个普通工程师，通过信任关系+ssh+rsync 基本可以管理10台服务器；但是如果要管理100台服务器，这个方法可能不灵验了，只有让一个OP工程师来搞定了；如果要管理1000台服务器，可能普通OP工程师也搞不定了，10个普通OP工程师也搞不定，想象一下不同的产品线，在不同数量的机器上上线，工程师之间的沟通成本，你可能需要一个运维团队使用不同的分工来搞定。如果是10000台，100000台机器，按不同的组来分工管理，也可以搞得定，但是成本一定不低，这个时候就需要机器自动化的管理方案了，很有技术含量。
* 之前做BCS，我们没有发生过一次雪崩的情况，不是我们比较牛B，而是：1. 机器的数目没达到，到不了雪崩的临界值；2. 自动化运维做的比较差，或者说没有自动化的迁移，所以无法雪崩；哪天如果真雪崩了，说明了BCS已经做的非常的前沿和牛B了。
* 将异常情况当作常态来测试，对做云相关的非常重要，但是真的很难做好。
* 做云还是很苦B的，对岸amazon的工程师，处理这个event，不是从白天10:00，处理到凌晨12:45PM么?

[english-link]: https://aws.amazon.com/message/680342/
[steveY对amazon和google的吐槽]: http://coolshell.cn/articles/5701.html
[steveY rant]: http://news.ycombinator.com/item?id=3102800
